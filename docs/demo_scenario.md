# Flux RAG 시연 시나리오

**대상**: 한국가스기술공사 (Korea Gas Technology Corporation)
**시연 시간**: 30분
**작성일**: 2026-02-27
**버전**: 2.0

---

## 목차

1. [시연 환경 준비 (사전)](#1-시연-환경-준비-사전)
2. [시나리오 1: ChatGPT 스타일 AI 채팅 (10분)](#2-시나리오-1-chatgpt-스타일-ai-채팅-10분)
3. [시나리오 2: 문서 업로드 & 인제스트 (5분)](#3-시나리오-2-문서-업로드--인제스트-5분)
4. [시나리오 3: 관리자 기능 (5분)](#4-시나리오-3-관리자-기능-5분)
5. [시나리오 4: 품질 관리 & 모니터링 (5분)](#5-시나리오-4-품질-관리--모니터링-5분)
6. [시나리오 5: 에이전트 & MCP (3분)](#6-시나리오-5-에이전트--mcp-3분)
7. [시나리오 6: 보안 & 컴플라이언스 (2분)](#7-시나리오-6-보안--컴플라이언스-2분)
8. [마무리 & Q&A](#8-마무리--qa)
9. [FAQ / 예상 질문 대응](#9-faq--예상-질문-대응)
10. [트러블슈팅](#10-트러블슈팅)

---

## 1. 시연 환경 준비 (사전)

### 1.1 시스템 요구사항

| 항목 | 요구사항 |
|------|---------|
| Python | 3.10 이상 |
| Node.js | 18 이상 |
| Ollama | 0.3+ (모델: qwen2.5:14b) |
| 메모리 | 16GB 이상 권장 |
| GPU | VRAM 12GB 이상 (14b 모델 사용 시) |

### 1.2 사전 체크리스트

시연 **30분 전**에 아래를 순서대로 확인합니다.

#### (1) Ollama 실행 확인

```bash
# Ollama 서버 실행
ollama serve

# 다른 터미널에서 모델 확인
curl http://localhost:11434/api/tags | python3 -m json.tool
# → qwen2.5:14b가 목록에 있어야 함

# 모델이 없으면 다운로드 (10~15분 소요)
ollama pull qwen2.5:14b
```

#### (2) 환경 변수 확인

`backend/.env` 파일에서 아래 설정 확인:

```bash
# LLM
DEFAULT_LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5:14b

# 인증 (시연용 — 비활성)
AUTH_ENABLED=false

# 벡터 DB
CHROMA_PERSIST_DIR=./data/chroma_db
CHROMA_COLLECTION_NAME=knowledge_base
```

> **주의**: `AUTH_ENABLED=false`여야 로그인 없이 바로 시연 가능합니다. 보안 시나리오에서 인증을 보여줄 때만 `true`로 전환합니다.

#### (3) Backend 시작

```bash
cd /Users/blockmeta/Desktop/workspace/flux-rag/backend
source .venv/bin/activate
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

시작 로그에서 아래 메시지를 확인합니다:
```
INFO: Warming up RAG pipeline...
INFO: RAG pipeline warm-up complete
INFO: Uvicorn running on http://0.0.0.0:8000
```

헬스 체크:
```bash
curl http://localhost:8000/health
# → {"status":"ok","app":"Flux RAG","version":"...","auth_enabled":false}
```

#### (4) Frontend 시작

```bash
cd /Users/blockmeta/Desktop/workspace/flux-rag/frontend
npm run dev
```

브라우저에서 확인: **http://localhost:5173**

#### (5) Ollama 모델 워밍업 (중요!)

서버가 모두 시작된 후, **Ollama 모델을 미리 로드**해야 합니다. 첫 요청 시 모델 로딩에 10~30초가 걸리므로, 시연 전에 반드시 워밍업합니다:

```bash
# 방법 1: curl로 직접 워밍업 (권장)
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "안녕하세요", "session_id": "warmup-session"}'

# 방법 2: Ollama에 직접 요청
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:14b", "prompt": "hello", "stream": false}'
```

응답이 정상적으로 올 때까지 기다립니다 (첫 워밍업: 10~30초, 이후 요청: 1~3초).

#### (6) 최종 점검표

| 체크 | 항목 | 확인 방법 |
|:----:|------|----------|
| [ ] | Ollama 실행 중 | `curl http://localhost:11434/api/tags` |
| [ ] | qwen2.5:14b 모델 로드됨 | 위 응답에 모델명 포함 |
| [ ] | Backend 실행 중 (포트 8000) | `curl http://localhost:8000/health` |
| [ ] | Frontend 실행 중 (포트 5173) | 브라우저에서 http://localhost:5173 |
| [ ] | AUTH_ENABLED=false | health 응답의 `auth_enabled: false` |
| [ ] | Ollama 모델 워밍업 완료 | 워밍업 curl 응답 수신 |
| [ ] | 벡터 DB 데이터 존재 | 39,739 chunks 이상 |

### 1.3 현재 데이터셋 현황

시연 시 아래 데이터가 이미 인제스트되어 있어야 합니다:

| 데이터셋 | 파일 수 | 청크 수 | 벤치마크 성공률 |
|---------|--------|--------|---------------|
| 한국가스공사법 | PDF + 관련법 | 117 (법률 28 + 정관 89) | **100%** (50/50) |
| 내부규정 | 676 HWP | 19,035 | **98.3%** (59/60) |
| 인쇄홍보물 | 7 PDF | 1,012 | **100%** (20/20) |
| ALIO 공시 | 403 files | 19,274 | **100%** (20/20) |
| 국외출장 보고서 | 100 files | 301 | **80%** (16/20) |
| **합계** | **1,186+ files** | **39,739 chunks** | **95.8%** (115/120) |

### 1.4 포트 정리

| 서비스 | 포트 | URL |
|--------|------|-----|
| Backend (FastAPI) | 8000 | http://localhost:8000 |
| Frontend (Vite) | 5173 | http://localhost:5173 |
| Ollama | 11434 | http://localhost:11434 |

---

## 2. 시나리오 1: ChatGPT 스타일 AI 채팅 (10분)

### 2.1 웰컴 화면 소개

#### 데모 순서

1. **브라우저에서 http://localhost:5173 접속**
   - ChatGPT 스타일의 깔끔한 채팅 인터페이스가 표시됨
   - 좌측: 대화 세션 사이드바 (이전 대화 목록)
   - 중앙: "Flux AI" 로고 + "무엇이든 물어보세요" 웰컴 메시지

2. **제안 칩(Suggestion Chips) 소개**
   - 웰컴 화면에 2x2 그리드로 4개의 제안 칩 표시:
     - "문서 내용을 요약해 주세요"
     - "보고서 작성을 도와주세요"
     - "데이터를 분석해 주세요"
     - "규정에 대해 설명해 주세요"
   - 칩 중 하나를 클릭하면 자동으로 입력창에 텍스트가 들어감

3. **UI 요소 설명**
   - 상단 바: 사이드바 토글, 모델 선택, 다크/라이트 모드 전환
   - 하단 입력창: 텍스트 입력 + 파일 첨부 + 전송 버튼

#### 강조 포인트
> "ChatGPT와 동일한 사용 경험을 제공합니다. 별도의 교육 없이 누구나 즉시 사용할 수 있습니다."

---

### 2.2 질문 1: 법률 도메인 (벤치마크 100% 영역)

#### 데모 순서

1. **입력창에 질문 입력**:

   > **"한국가스공사법에서 정한 가스공사의 주요 사업은 무엇인가요?"**

2. **전송 후 관찰 포인트**:
   - SSE 스트리밍: 토큰이 실시간으로 하나씩 표시됨
   - 응답 완료 후 소스 카드(참고 문서) 표시
   - 신뢰도 배지: **"신뢰도: 높음 (9x%)"** (녹색)

3. **소스 카드 클릭**:
   - 카드를 클릭하면 해당 청크의 원문 프리뷰가 펼쳐짐
   - 파일 아이콘으로 문서 유형 구분 (PDF, HWP 등)
   - 유사도 점수 표시

#### 예상 답변 (요약)
```
한국가스공사법에 따른 주요 사업은 다음과 같습니다:

1. 천연가스의 인수, 저장, 가공 및 공급
2. 천연가스 공급시설의 건설 및 운영
3. 해외 천연가스 개발 사업
4. 가스 관련 연구개발 및 기술지원
...

[출처: 한국가스공사법 관련 조문]
```

#### 강조 포인트
> "한국가스공사법 벤치마크에서 **50문항 전량 정답(100%)**을 달성한 도메인입니다. 법률 조문에서 정확하게 정보를 추출합니다."

---

### 2.3 질문 2: 내부규정 도메인 (벤치마크 98.3%)

#### 데모 순서

1. **새 질문 입력**:

   > **"연차휴가 일수는 근속연수에 따라 어떻게 달라지나요?"**

2. **관찰 포인트**:
   - 내부규정(HWP) 문서에서 관련 규정을 검색
   - 근속연수별 연차 일수를 구조화하여 답변
   - 소스 카드에 HWP 파일 아이콘 표시

#### 예상 답변 (요약)
```
복무규정에 따른 연차휴가 일수는 다음과 같습니다:

- 1년 미만: 월 1일 (최대 11일)
- 1년 이상 ~ 3년 미만: 15일
- 3년 이상: 매 2년마다 1일 가산 (최대 25일)
...

[출처: 복무규정 관련 조항]
```

#### 강조 포인트
> "676건의 HWP 내부규정 문서(19,035 청크)를 검색합니다. 벤치마크 **98.3% 정확도**를 달성했습니다."

---

### 2.4 질문 3: ALIO 공시 교차분석 (벤치마크 100%)

#### 데모 순서

1. **질문 입력**:

   > **"ALIO 공시 자료에서 한국가스기술공사의 최근 매출 추이를 알려주세요"**

2. **관찰 포인트**:
   - ALIO(공공기관 경영정보 공개시스템) 데이터에서 재무 정보 검색
   - source_type 자동 필터링: "ALIO" 키워드 감지 → ALIO 데이터셋 우선 검색
   - 수치 데이터 포함 답변

#### 강조 포인트
> "질문에 포함된 키워드('ALIO', '공시')를 분석하여 자동으로 해당 데이터셋을 우선 검색합니다. ALIO 공시 벤치마크 **20문항 전량 정답(100%)**입니다."

---

### 2.5 질문 4: 안전장치 테스트 (범위 외 질문)

#### 데모 순서

1. **의도적으로 범위 외 질문 입력**:

   > **"양자역학에 대해 설명해주세요"**

2. **관찰 포인트**:
   - 신뢰도 배지: **"신뢰도: 낮음"** (빨간색)
   - 안전 메시지 표시: "제공된 문서에서 관련 정보를 찾을 수 없습니다"
   - 답변 가능한 주제 영역 안내

#### 예상 답변 (요약)
```
죄송합니다. 제공된 문서에서 양자역학과 관련된 정보를 찾을 수 없습니다.

저는 한국가스기술공사의 업무 문서를 기반으로 답변하는 AI 어시스턴트입니다.
다음과 같은 주제에 대해 질문해 주세요:
- 한국가스공사법 및 관련 법령
- 내부규정 (인사, 복무, 보수 등)
- ALIO 공시 자료
- 홍보물 및 출장 보고서
```

#### 강조 포인트
> "보유 문서 범위 밖의 질문에는 무리하게 답변하지 않고, 안전하게 거절합니다. **환각(hallucination) 방지**가 핵심입니다."

---

### 2.6 부가 기능 시연

#### (1) 다크/라이트 모드 전환

1. 상단 바의 다크/라이트 모드 토글 클릭
2. 즉시 전체 UI 테마 전환 확인
3. 다시 원래 모드로 복귀

#### (2) 대화 복사

1. 기존 답변의 복사 아이콘 클릭
2. 클립보드에 마크다운 형식으로 복사됨

#### (3) 메시지 편집 + 재전송

1. 이전에 보낸 사용자 메시지의 편집 아이콘 클릭
2. 질문 텍스트를 수정
3. 재전송하면 수정된 질문으로 새 답변 생성

#### (4) 스트리밍 중단

1. 긴 답변이 예상되는 질문 입력 (예: "가스 안전 교육 프로그램을 상세히 설명해주세요")
2. 응답 생성 중 **중단(Stop)** 버튼 클릭
3. 즉시 스트리밍이 멈추고 생성된 내용까지만 표시

#### 강조 포인트
> "ChatGPT에서 익숙한 모든 기능 — 다크모드, 복사, 편집, 중단 — 을 동일하게 지원합니다."

---

## 3. 시나리오 2: 문서 업로드 & 인제스트 (5분)

### 데모 순서

#### (1) Documents 페이지 이동

1. 좌측 또는 상단 네비게이션에서 **"문서 관리"** 클릭
2. 이미 인제스트된 문서 목록 확인:
   - 파일명, 업로드 일시, 청크 수, 파일 크기, source_type
   - 현재 **39,739 청크** 보유 상태 확인

#### (2) Drag & Drop 문서 업로드

1. 시연용 PDF 파일을 준비 (아무 한국어 PDF 1~2장짜리)
2. 파일을 업로드 영역에 **Drag & Drop**
3. 실시간 인제스트 진행 관찰:
   - 파일 업로드 -> 텍스트 추출 -> 청킹 -> 임베딩 생성 -> 벡터 DB 저장
   - 진행 상태(progress) 표시

> **API 경로**: `POST /api/documents/upload` (멀티파트 파일 업로드)

#### (3) 업로드 후 즉시 검색 확인

1. 인제스트 완료 후 Chat 페이지로 이동
2. 방금 업로드한 문서 내용에 대해 질문
3. 새로 인제스트된 문서가 소스에 나타나는지 확인

#### 강조 포인트
> "문서 업로드에서 질의응답까지 **코딩 없이, 클릭만으로** 완료됩니다. PDF, HWP, DOCX, XLSX, PPTX, TXT 등 **6종 문서 포맷**을 지원합니다."

#### 예상 질문
- **Q: HWP 파일도 지원하나요?**
  - A: 네, pyhwp 라이브러리를 통해 HWP 파일을 직접 파싱합니다. 실제로 내부규정 676건이 모두 HWP 파일이며, 정상적으로 인제스트되어 있습니다.

- **Q: 대용량 문서는 어떻게 처리하나요?**
  - A: 자동 청킹(chunking)으로 문서를 적절한 크기로 분할합니다. 비동기 배치 처리로 동시에 5개 파일까지 병렬 인제스트할 수 있습니다.

---

## 4. 시나리오 3: 관리자 기능 (5분)

### 데모 순서

#### (1) Admin 페이지 이동

1. 네비게이션에서 **"관리자"** 클릭
2. Admin 페이지에는 **9개 탭**이 있음:
   - 시스템 정보 / 프로바이더 / 모델 / 가드레일 / 프롬프트 / MCP / 커스텀 도구 / 워크플로 / 콘텐츠

#### (2) 시스템 정보 탭

1. **시스템 정보** 탭 선택
2. 표시 항목:
   - 임베딩 모델: `BAAI/bge-m3` (1024 차원)
   - 벡터 DB: ChromaDB (39,739 chunks)
   - LLM 프로바이더: Ollama (qwen2.5:14b)
   - 메모리 사용량, 서버 가동 시간

#### 강조 포인트
> "시스템 전체 상태를 한눈에 파악할 수 있습니다. 운영 중 문제 발생 시 빠르게 원인을 진단할 수 있습니다."

#### (3) 모델 관리 탭

1. **모델** 탭 선택
2. LLM 모델 레지스트리 확인:
   - 등록된 모델 목록 (Ollama, OpenAI, Anthropic, vLLM)
   - 현재 활성 모델 표시
   - 모델 추가/수정/삭제 가능

3. **프로바이더 전환 시연** (선택):
   - Ollama → OpenAI 전환 가능 (API Key 설정 시)
   - "코드 수정 없이 설정만으로 LLM 교체 가능"

#### 강조 포인트
> "4개 LLM 프로바이더를 지원합니다. 망분리 환경에서는 Ollama/vLLM, 클라우드 환경에서는 OpenAI/Anthropic을 선택할 수 있습니다."

#### (4) 가드레일 탭 — 규칙 추가 시연

1. **가드레일** 탭 선택
2. 현재 등록된 가드레일 규칙 목록 확인
3. **새 규칙 추가 시연**:
   - "추가" 버튼 클릭
   - 규칙 유형: **키워드 차단**
   - 키워드: `비밀번호`
   - 설명: "비밀번호 관련 질문 차단"
   - 저장

4. **규칙 테스트**:
   - Chat 페이지로 이동
   - 질문 입력: "관리자 비밀번호를 알려주세요"
   - 가드레일에 의해 **차단** 또는 **경고 메시지** 표시 확인

5. (시연 후 규칙 삭제 또는 비활성화)

#### 강조 포인트
> "가드레일로 민감한 키워드, 프롬프트 인젝션 공격, 부적절한 입출력을 **실시간으로 차단**합니다. 관리자가 코딩 없이 규칙을 추가/수정할 수 있습니다."

#### (5) 프롬프트 편집

1. **프롬프트** 탭 선택
2. 현재 System Prompt 내용 확인
3. 일부 텍스트를 수정 (예: 인사말 변경)
4. **저장** → "프롬프트 변경이 즉시 적용됩니다"

#### 강조 포인트
> "시스템 프롬프트를 실시간으로 수정할 수 있습니다. 프롬프트 버저닝으로 이전 버전으로 **롤백**도 가능합니다."

---

## 5. 시나리오 4: 품질 관리 & 모니터링 (5분)

### 데모 순서

#### (1) Quality Dashboard 페이지

1. 네비게이션에서 **"품질 대시보드"** 클릭
2. 3가지 품질 분석 탭:

   **a. 청크 품질 분석**
   - 청크 길이 분포 히스토그램
   - 평균/중앙값/최대/최소 청크 길이
   - 중복 청크 탐지 결과
   - 의미 완결성 점수

   **b. 임베딩 추적**
   - 인제스트 작업별 진행 상태
   - 임베딩 생성 성공/실패 카운트
   - 최근 인제스트 이력

   **c. 벡터 분포 분석**
   - 벡터 유사도 분포
   - 클러스터링 시각화
   - 이상치(outlier) 탐지

#### 강조 포인트
> "RAG 시스템의 품질을 **정량적으로 관리**합니다. 청크가 너무 짧거나 중복이 많으면 검색 품질이 떨어지므로, 대시보드에서 지속적으로 모니터링합니다."

#### (2) Monitor 페이지

1. 네비게이션에서 **"모니터링"** 클릭
2. 대시보드 항목:
   - 총 질문 수, 평균 응답 시간
   - 일별/주별 사용 추이 그래프
   - 신뢰도 분포 (높음/중간/낮음 비율)
   - 사용 통계 Excel 내보내기 가능

#### (3) 피드백 루프 시연

1. **Chat 페이지**로 돌아가기
2. 이전 답변에서 **좋아요(Thumbs Up)** 클릭 → "피드백 감사합니다" 메시지
3. 다른 답변에서 **싫어요(Thumbs Down)** 클릭:
   - 피드백 코멘트 입력: "출처가 더 구체적이면 좋겠습니다"
   - 제출

4. (Admin 역할인 경우) **골든 데이터 등록**:
   - 답변 옆의 "골든 데이터" 아이콘 클릭
   - 정확한 답변 내용 편집 + 평가 태그 선택 (factual, inference 등)
   - 저장 → 벤치마크 평가 데이터로 활용

#### 강조 포인트
> "사용자 피드백이 **품질 개선 루프**로 이어집니다. 피드백 → 분석 → 프롬프트/데이터 개선 → 재평가의 선순환 구조입니다."

---

## 6. 시나리오 5: 에이전트 & MCP (3분)

### 데모 순서

#### (1) MCP 도구 목록

1. Admin 페이지 → **MCP** 탭 선택
2. 내장 도구 10개 확인:

   | 도구 | 기능 |
   |------|------|
   | calculator | 수학 계산 |
   | search | 웹 검색 |
   | knowledge_base | 지식 베이스 조회 |
   | summarizer | 문서 요약 |
   | translator | 번역 |
   | report_generator | 보고서 생성 |
   | email_composer | 이메일 작성 |
   | data_analyzer | 데이터 분석 |
   | + 2개 추가 | 도메인 특화 도구 |

3. 도구 실행 테스트 (calculator 등 간단한 도구)

#### (2) 커스텀 도구 빌더

1. Admin 페이지 → **커스텀 도구** 탭 선택
2. "노코드 도구 빌더" 소개:
   - 도구명, 설명, 입력 파라미터, 실행 로직 정의
   - 코딩 없이 새로운 도구 생성 가능

3. (시연 시간이 허용되면) 간단한 커스텀 도구 생성 데모

#### (3) Agent Builder 소개 (간략)

1. 네비게이션에서 **"에이전트 빌더"** 클릭
2. React Flow 기반 **비주얼 캔버스** 소개:
   - 노드를 드래그하여 워크플로 구성
   - 도구 노드, 조건 노드, LLM 노드 연결
   - 3개 프리셋 워크플로 제공

#### 강조 포인트
> "MCP(Model Context Protocol) 기반으로 AI 에이전트의 도구 사용 능력을 확장합니다. 에이전트 빌더로 **복잡한 업무를 자동화하는 워크플로**를 코딩 없이 구성할 수 있습니다."

---

## 7. 시나리오 6: 보안 & 컴플라이언스 (2분)

### 데모 순서 (설명 위주, 슬라이드 보조 권장)

#### (1) JWT 인증 체계

- 구현 완료: JWT(JSON Web Token) 기반 인증
- Access Token + Refresh Token 이중 구조
- 토큰 타입 검증으로 Refresh → Access 악용 차단

#### (2) RBAC 4역할 접근 제어

| 역할 | 권한 |
|------|------|
| **Admin** | 전체 기능 + 시스템 관리 + 사용자 관리 |
| **Manager** | 문서 관리 + 모니터링 + 피드백 관리 |
| **User** | 채팅 + 문서 조회 |
| **Viewer** | 읽기 전용 |

#### (3) 보안 기능 요약

| 기능 | 설명 |
|------|------|
| 가드레일 | 입출력 필터링, 프롬프트 인젝션 탐지, 키워드 차단 |
| 감사 로그 | 모든 인증 이벤트 기록 (로그인, 비밀번호 변경 등) |
| 남용 탐지 | IP 블랙리스트, 이상 패턴 탐지, 레이트 리밋 |
| PII 탐지 | 한국형 개인정보 탐지 (주민번호, 전화번호 등) |
| 비밀번호 정책 | 최소 8자, 대문자+숫자+특수문자, 최초 로그인 시 변경 강제 |
| 보안 헤더 | X-Content-Type-Options, X-Frame-Options, CSP 등 |

#### (선택) 인증 라이브 시연

> 시간이 허용되면 `.env`에서 `AUTH_ENABLED=true`로 변경 후 서버 재시작하여 시연할 수 있습니다.

1. 로그인 페이지 표시 확인
2. 테스트 계정으로 로그인:

   | 역할 | ID | PW |
   |------|-----|-----|
   | Admin | admin | admin123 |
   | Manager | manager | manager123 |
   | User | user | user123 |

3. 역할별 메뉴 접근 권한 차이 확인:
   - User: 채팅만 가능, Admin 메뉴 접근 불가
   - Admin: 전체 메뉴 접근 가능

#### 강조 포인트
> "엔터프라이즈 수준의 보안을 갖추고 있습니다. 인증, 역할 기반 접근 제어, 감사 로그, 남용 탐지, PII 보호까지 **컴플라이언스 요구사항을 충족**합니다."

---

## 8. 마무리 & Q&A

### 벤치마크 결과 요약

```
+--------------------+--------+----------+
| 데이터셋           | 문항 수 | 성공률   |
+--------------------+--------+----------+
| 한국가스공사법      |   50   | 100.0%   |
| 내부규정           |   60   |  98.3%   |
| 인쇄홍보물         |   20   | 100.0%   |
| ALIO 공시          |   20   | 100.0%   |
| 국외출장 보고서     |   20   |  80.0%   |
+--------------------+--------+----------+
| 합계               |  120   |  95.8%   |
+--------------------+--------+----------+
```

- **모델**: qwen2.5:14b (Ollama, temperature 0.1)
- **임베딩**: BAAI/bge-m3 (1024 차원)
- **검색**: Hybrid (Vector + BM25 + FlashRank 리랭킹)
- **총 벡터 DB 규모**: 39,739 chunks

### 핵심 차별점 요약

| 항목 | Flux RAG |
|------|----------|
| 정확도 | 120문항 벤치마크 95.8% |
| 검색 방식 | 하이브리드 (벡터 + BM25 + 리랭킹) |
| LLM 유연성 | 4개 프로바이더 즉시 전환 |
| 문서 포맷 | PDF, HWP, DOCX, XLSX, PPTX, TXT (6종) |
| 보안 | JWT + RBAC + 가드레일 + 감사 + PII |
| 확장성 | 에이전트 빌더, MCP 도구, 커스텀 도구 빌더 |
| 망분리 대응 | Ollama/vLLM로 완전 오프라인 운영 가능 |
| 운영 도구 | 품질 대시보드, 모니터링, 통계 Excel 내보내기 |

### 확장 로드맵

| 시기 | 계획 |
|------|------|
| 즉시 | 출장보고서 OCR 재인제스트 (Upstage Document Parse) |
| 단기 (1~2개월) | vLLM 운영 서빙, K8s 배포, Redis 캐시 |
| 중기 (3~6개월) | 사내 ERP 연동, 멀티모달 (도면/이미지), 모바일 앱 |
| 장기 | 한국가스기술공사 특화 파인튜닝 모델, 멀티테넌트 |

### 배포 환경

- **권장**: RunPod A40 Community Cloud ($0.35/hr, 월 약 $259)
- GCP 대비 3~5배 저렴, GPU VRAM 48GB로 14b 모델 여유 있게 서빙

### 맺음말

> "Flux RAG는 단순한 질의응답 시스템이 아닙니다.
> 한국가스기술공사의 법률, 규정, 공시, 보고서 등 **전사 지식을 지능적으로 연결**하여
> 직원 여러분의 업무 효율을 획기적으로 높이는 **종합 엔터프라이즈 AI 플랫폼**입니다.
>
> 오늘 시연한 채팅, 문서 관리, 가드레일, 품질 관리, 에이전트에 더해
> MCP 도구와 Agent Builder를 통해 **무한한 업무 자동화 가능성**을 가지고 있습니다."

---

## 9. FAQ / 예상 질문 대응

### Q1: 왜 여러 LLM 모델을 교체할 수 있어야 하나요?

**A**:
- **망분리 환경 대응**: 인터넷 차단 환경에서는 Ollama/vLLM 로컬 모델 필수
- **비용 최적화**: 단순 질문은 로컬 7b 모델, 복잡한 분석은 14b 또는 클라우드 모델로 분리
- **벤더 종속 회피**: 특정 LLM 벤더에 종속되지 않고 언제든 전환 가능
- **성능 비교**: 동일 벤치마크로 모델 간 정확도/속도 비교 평가 가능

### Q2: 벤치마크 95.8%의 나머지 4.2%는 왜 실패했나요?

**A**:
- 실패 5건 중 4건은 **국외출장 보고서** 데이터셋에서 발생
- 원인: 스캔된 PDF의 텍스트 추출 품질 이슈 (OCR 미적용)
- 해결책: Upstage Document Parse OCR 재인제스트 예정
- 나머지 1건은 내부규정 60문항 중 다중 규정 교차 추론 문제

### Q3: 신뢰도 점수는 어떻게 계산되나요?

**A**:
- 검색된 Top-K 청크의 유사도 점수 기반
- 3단계 분류:
  - **높음(녹색)**: 0.8 이상 — 문서에서 직접 근거를 찾음
  - **중간(노란색)**: 0.5 ~ 0.8 — 관련 정보 존재하나 직접 근거 약함
  - **낮음(빨간색)**: 0.5 미만 — 문서에 관련 정보 부족, 안전 메시지 표시

### Q4: 보안은 어떻게 관리하나요?

**A**:
- **인증**: JWT 기반 (Access + Refresh Token), 토큰 타입 검증
- **권한**: RBAC 4역할 (Admin/Manager/User/Viewer)
- **가드레일**: 프롬프트 인젝션 탐지, 키워드/정규식 필터, 입출력 길이 제한
- **감사**: 모든 인증 이벤트, 질의응답 기록 로깅
- **PII 보호**: 한국형 개인정보 (주민번호, 전화번호, 이메일 등) 자동 탐지
- **남용 방지**: IP 블랙리스트, 레이트 리밋, 이상 패턴 탐지
- **망분리**: Ollama/vLLM으로 외부 통신 없이 완전 자체 운영 가능

### Q5: 실제 데이터로 전환할 때 추가 작업이 필요한가요?

**A**:
- 현재 시연에 사용된 데이터가 **이미 실제 한국가스기술공사 문서**입니다
- 추가 문서 인제스트: Documents 페이지에서 Drag & Drop 업로드
- 시스템 프롬프트 커스터마이징: Admin 페이지에서 실시간 수정
- 문서 동기화: 설정 시 새 문서 자동 감지 + 재인제스트 가능

### Q6: 동시 사용자를 얼마나 처리할 수 있나요?

**A**:
- **FastAPI 비동기**: 수천 개 동시 연결 처리 가능
- **LLM 병목**: Ollama 단일 인스턴스 기준 동시 5~10 요청
- **vLLM 운영**: continuous batching으로 동시 50+ 요청 처리
- **수평 확장**: K8s로 Backend + LLM 인스턴스 스케일 아웃
- **캐싱**: Redis 캐시로 반복 질문 응답 속도 10배 향상

### Q7: 기존 사내 시스템과 통합할 수 있나요?

**A**:
- **REST API**: 모든 기능이 API로 제공 → 어떤 시스템과도 연동 가능
- **MCP 도구**: 사내 ERP, 설비 관리 시스템을 Tool로 등록하여 에이전트가 직접 호출
- **SSO 연동**: LDAP, OAuth2 지원 (인증 모듈 이미 구현)
- **Webhook**: 이벤트 기반 외부 시스템 알림 확장 가능

### Q8: 답변이 틀렸을 때 어떻게 개선하나요?

**A**:
- **즉시**: 사용자 피드백(싫어요 + 코멘트) 수집
- **분석**: Monitor 페이지에서 부정 피드백 패턴 분석
- **개선 경로**:
  1. 문서 부족 → 관련 문서 추가 인제스트
  2. 프롬프트 미흡 → Admin에서 System Prompt 수정 (즉시 적용)
  3. 골든 데이터 등록 → 벤치마크 평가 기준에 추가
  4. 모델 교체 → 더 강력한 LLM으로 전환

---

## 10. 트러블슈팅

### 이슈 1: Frontend에서 "서버 연결 실패"

**원인**: Backend가 실행 중이지 않거나 포트 불일치

**해결**:
```bash
# Backend 실행 확인
curl http://localhost:8000/health

# 실행 중이지 않다면
cd /Users/blockmeta/Desktop/workspace/flux-rag/backend
source .venv/bin/activate
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### 이슈 2: 첫 질문 응답이 매우 느림 (10~30초)

**원인**: Ollama 모델이 메모리에 로드되지 않은 상태

**해결**:
```bash
# Ollama 모델 워밍업
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2.5:14b", "prompt": "hello", "stream": false}'
```

> 워밍업 후에는 1~3초 내에 응답이 시작됩니다.

### 이슈 3: "검색 결과 없음" 또는 매우 낮은 신뢰도

**원인**: 벡터 DB에 데이터가 없거나 인제스트되지 않음

**해결**:
```bash
# 벡터 DB 청크 수 확인
curl http://localhost:8000/api/admin/system-info
# → total_chunks 확인 (39,739 이상이어야 함)

# 데이터가 없으면 재인제스트 필요
cd /Users/blockmeta/Desktop/workspace/flux-rag/backend
python scripts/ingest_all.py
```

### 이슈 4: Ollama 연결 실패

**원인**: Ollama 서버가 실행 중이지 않음

**해결**:
```bash
# Ollama 상태 확인
curl http://localhost:11434/api/tags

# 실행 중이지 않다면
ollama serve

# 모델 확인 / 다운로드
ollama list
ollama pull qwen2.5:14b
```

### 이슈 5: ChromaDB 오류 (database is locked)

**원인**: 이전 프로세스가 DB 잠금을 해제하지 않음

**해결**:
```bash
# Backend 서버 중지
# ChromaDB 잠금 초기화
rm -rf /Users/blockmeta/Desktop/workspace/flux-rag/backend/data/chroma_db/.lock
# Backend 재시작
```

> **주의**: `chroma_db` 폴더 전체를 삭제하면 모든 데이터가 소실됩니다. `.lock` 파일만 삭제하세요.

### 이슈 6: Frontend 빌드/실행 실패

**원인**: node_modules 미설치 또는 버전 불일치

**해결**:
```bash
cd /Users/blockmeta/Desktop/workspace/flux-rag/frontend
rm -rf node_modules
npm install
npm run dev
```
