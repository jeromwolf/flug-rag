# flux-rag 시연 시나리오

**대상**: 한국가스기술공사 (Korea Gas Technology Corporation)
**시연 시간**: 30분
**작성일**: 2026-02-05

---

## 1. 시연 환경 준비

### 1.1 사전 요구사항
- Python 3.10 이상
- Node.js 18 이상
- 환경 변수 설정 (`backend/.env`)
  - `EMBEDDING_MODEL=BAAI/bge-m3`
  - `LLM_PROVIDER=ollama` (또는 `openai`, `anthropic`)
  - `OLLAMA_BASE_URL=http://localhost:11434` (Ollama 사용 시)
  - `CHROMA_DB_PATH=./data/chroma_db`

### 1.2 서비스 시작

#### Backend 시작
```bash
cd /Users/blockmeta/Desktop/workspace/flux-rag/backend
uvicorn api.main:app --reload --port 8000
```

**확인**: http://localhost:8000/health (200 OK 응답)

#### Frontend 시작
```bash
cd /Users/blockmeta/Desktop/workspace/flux-rag/frontend
npm run dev
```

**확인**: http://localhost:3000 (React 앱 로딩)

### 1.3 샘플 문서 인제스트

**샘플 문서 5종**:
1. `가스안전관리_규정.pdf` - 안전관리 기본 규정
2. `설비점검_매뉴얼.pdf` - 일일/주간/월간 점검 절차
3. `비상대응_절차서.pdf` - 가스 누출 시 대응 매뉴얼
4. `월간보고서_2024_01.pdf` - 2024년 1월 점검 실적 보고
5. `교육훈련_계획서.pdf` - 연간 교육 계획

**인제스트 방법**:
- Documents 페이지에서 Drag & Drop 업로드
- 또는 Backend API 직접 호출:
  ```bash
  curl -X POST "http://localhost:8000/api/documents/ingest" \
       -F "files=@/path/to/가스안전관리_규정.pdf"
  ```

---

## 2. 시연 시나리오 (30분)

### 시나리오 1: 문서 업로드 및 관리 (5분)

#### 데모 순서
1. **Documents 페이지 열기**
   - 좌측 네비게이션에서 "문서 관리" 클릭

2. **Drag & Drop 업로드**
   - 3개 문서 선택 (가스안전관리 규정, 설비점검 매뉴얼, 비상대응 절차서)
   - 드래그하여 업로드 영역에 놓기
   - 업로드 진행 상태 표시 확인

3. **문서 목록 확인**
   - 업로드된 문서 리스트 표시
   - 각 문서의 메타데이터 표시:
     - 파일명
     - 업로드 일시
     - 청크(chunk) 개수
     - 파일 크기

4. **강조 포인트**
   - "PDF, DOCX, TXT 등 다양한 포맷 지원"
   - "자동 청킹(chunking)으로 대용량 문서 처리"
   - "메타데이터 기반 문서 관리"

---

### 시나리오 2: 지능형 질의응답 (10분)

#### 질문 1: 간단한 사실 확인 (난이도: 하)
**질문**: "가스시설 일일 점검 시간은 언제인가요?"

**예상 답변**:
```
가스시설 일일 점검은 다음 시간에 수행됩니다:
- 오전 점검: 09:00
- 오후 점검: 17:00

[출처: 설비점검_매뉴얼.pdf, 3.1절 일일점검 시간]
```

**시연 포인트**:
- ✅ **신뢰도 배지**: "높은 신뢰도" (녹색)
- ✅ **출처 하이라이트**: 해당 청크 강조 표시
- ✅ **스트리밍 응답**: 토큰이 실시간으로 나타남

---

#### 질문 2: 절차적 질문 (난이도: 중)
**질문**: "가스 누출 발견 시 비상대응 절차를 단계별로 설명해주세요"

**예상 답변**:
```
가스 누출 발견 시 비상대응 절차는 다음과 같습니다:

1단계: 감지 및 확인
- 가스 검지기 경보 확인
- 육안 및 후각으로 누출 지점 파악
- 즉시 통제실에 상황 보고

2단계: 경보 발령
- 비상벨 작동 (3회 장음)
- 무선통신으로 전 직원에게 알림
- 인근 주민에게 안내방송

3단계: 대피 유도
- 누출 지점 반경 50m 이내 대피
- 화기 사용 즉시 중단
- 비상 출구로 신속 이동

4단계: 초기 조치
- 누출 밸브 차단 (안전한 경우에만)
- 환기 실시
- 소방서 및 한국가스안전공사에 신고

[출처: 비상대응_절차서.pdf, 4장 가스누출 대응]
```

**시연 포인트**:
- ✅ **다중 청크 통합**: 여러 단계가 하나의 일관된 답변으로 구성
- ✅ **구조화된 답변**: 단계별 번호 매기기
- ✅ **출처 표시**: 단일 문서에서 여러 섹션 참조

---

#### 질문 3: 교차 문서 질문 (난이도: 중)
**질문**: "2024년 1월에 발견된 설비 이상 사항과 조치 결과를 알려주세요"

**예상 답변**:
```
2024년 1월 설비 점검에서 발견된 이상 사항은 다음과 같습니다:

1. B동 배관 플랜지 누출 (1월 12일)
   - 증상: 미세 가스 누출 감지
   - 조치: 플랜지 볼트 재체결 및 가스켓 교체
   - 결과: 누출 완전 차단 확인

2. A동 압력조정기 이상 (1월 18일)
   - 증상: 출구 압력 0.2kPa 초과
   - 조치: 압력조정기 교정 및 스프링 교체
   - 결과: 정상 범위(±0.1kPa) 복귀

3. C동 가스검지기 오작동 (1월 25일)
   - 증상: 무작위 경보 발생
   - 조치: 센서 청소 및 교정
   - 결과: 정상 작동 확인

[출처: 월간보고서_2024_01.pdf, 5장 이상사항 및 조치내역]
```

**시연 포인트**:
- ✅ **시간 기반 검색**: "2024년 1월" 키워드 필터링
- ✅ **신뢰도 점수**: 중간~높음 (문서에서 직접 추출)
- ✅ **출처 검증**: 특정 월간보고서 참조

---

#### 질문 4: 복잡한 분석 질문 (난이도: 상)
**질문**: "안전점검 매뉴얼의 기밀시험 기준과 실제 월간 보고서의 점검 결과를 비교해주세요"

**예상 답변**:
```
기밀시험 기준과 실제 점검 결과를 비교하면 다음과 같습니다:

[매뉴얼 기준]
- 시험 압력: 사용 압력의 1.5배
- 유지 시간: 최소 30분
- 허용 압력강하: 0.5kPa 이하
- 시험 주기: 연 2회 (6월, 12월)

[2024년 1월 실제 점검 결과]
- A동 배관: 시험 압력 30kPa (사용압력 20kPa × 1.5) ✅
- 유지 시간: 35분 ✅
- 압력강하: 0.3kPa ✅ (기준 내)
- 판정: 합격

[비교 분석]
✅ 모든 항목이 매뉴얼 기준을 충족합니다.
✅ 압력강하가 허용치의 60% 수준으로 양호한 상태입니다.

[출처]
- 설비점검_매뉴얼.pdf, 6.2절 기밀시험 기준
- 월간보고서_2024_01.pdf, 3.3절 기밀시험 결과
```

**시연 포인트**:
- ✅ **교차 문서 분석**: 2개 문서에서 정보 통합
- ✅ **비교 테이블**: 기준 vs 실제 구조화
- ✅ **다중 출처 표시**: 각 정보의 출처 명확히 표시

---

#### 질문 5: 안전장치 테스트 (범위 외 질문)
**질문**: "양자역학에 대해 설명해주세요"

**예상 답변**:
```
⚠️ 신뢰도 낮음

죄송합니다. 제공된 문서에서 해당 질문과 관련된 정보를 찾을 수 없습니다.

저는 한국가스기술공사의 가스 안전 관리 문서를 기반으로 답변하는 AI 어시스턴트입니다.
다음과 같은 주제에 대해 질문해 주세요:
- 가스 안전 관리 규정
- 설비 점검 절차
- 비상 대응 매뉴얼
- 교육 및 훈련 계획

도움이 필요하시면 관련 주제로 다시 질문해 주세요.
```

**시연 포인트**:
- ✅ **안전장치 작동**: 문서 범위 외 질문 감지
- ✅ **낮은 신뢰도 표시**: 빨간색 배지 "신뢰도 낮음"
- ✅ **안내 메시지**: 답변 가능한 주제 명확히 제시
- ✅ **한국어 최적화**: 자연스러운 한국어 거절 메시지

---

### 시나리오 3: 실시간 스트리밍 시연 (3분)

#### 데모 순서
1. **긴 답변 질문 입력**
   - 예: "가스 안전 교육 프로그램을 단계별로 상세히 설명해주세요"

2. **스트리밍 효과 관찰**
   - 토큰이 하나씩 실시간으로 나타남
   - 응답 생성 중 애니메이션 표시

3. **중단 기능 테스트**
   - 응답 생성 중 "중단" 버튼 클릭
   - 즉시 스트리밍 중단 확인

**강조 포인트**:
- "Server-Sent Events (SSE) 기반 실시간 스트리밍"
- "사용자 경험 향상 - 응답을 기다리는 답답함 해소"
- "언제든 중단 가능 - 불필요한 응답 방지"

---

### 시나리오 4: 모델 교체 시연 (3분)

#### 데모 순서
1. **설정 페이지 이동**
   - 우측 상단 설정 아이콘 클릭

2. **LLM Provider 변경**
   - 현재: Ollama (로컬 모델)
   - 변경: OpenAI GPT-4 또는 Anthropic Claude

3. **동일 질문 재시도**
   - 질문: "가스 누출 발견 시 초기 대응은?"
   - Ollama 응답과 OpenAI/Claude 응답 비교

4. **차이점 설명**
   - Ollama: 빠른 응답, 망분리 환경 대응
   - OpenAI/Claude: 더 정교한 추론, 복잡한 질문에 강함

**강조 포인트**:
- "3개 LLM Provider 지원 (Ollama, OpenAI, Anthropic)"
- "망분리 환경에서는 Ollama/vLLM 로컬 모델 사용"
- "클라우드 모델로 성능 비교 및 벤치마킹 가능"
- "설정 변경만으로 즉시 전환 - 코드 수정 불필요"

---

### 시나리오 5: 질문 라우팅 시각화 (3분)

#### 데모 순서
1. **일상 대화 (Chitchat)**
   - 질문: "안녕하세요"
   - 예상 라우팅: `CHITCHAT`
   - 응답: 간단한 인사 메시지

2. **문서 검색 (Document Search)**
   - 질문: "가스 배관 점검 기준은?"
   - 예상 라우팅: `DOCUMENT_SEARCH`
   - 응답: RAG 파이프라인 실행 → 문서 기반 답변

3. **복잡한 작업 (Complex Task)**
   - 질문: "A동과 B동의 점검 결과를 비교 분석해줘"
   - 예상 라우팅: `COMPLEX_TASK`
   - 응답: 다중 문서 통합 분석

**강조 포인트**:
- "지능형 라우터가 질문 유형을 자동 분류"
- "불필요한 RAG 파이프라인 실행 방지 → 비용 절감"
- "복잡한 질문은 향후 Agent Builder로 확장 가능"

---

### 시나리오 6: 관리자 기능 (3분)

#### 6.1 Admin 페이지
1. **시스템 정보**
   - 현재 사용 중인 임베딩 모델 표시
   - 벡터 DB 상태 (문서 수, 청크 수)
   - 메모리 사용량

2. **Provider 목록**
   - 사용 가능한 LLM Provider 리스트
   - 각 Provider의 모델명 표시
   - 현재 활성 Provider 강조

3. **프롬프트 편집기**
   - System Prompt 실시간 수정
   - 프롬프트 템플릿 관리
   - 저장 및 즉시 적용

**강조 포인트**:
- "관리자 페이지에서 모든 설정 중앙 관리"
- "프롬프트 수정으로 답변 스타일 조정 가능"
- "코드 배포 없이 설정 변경 가능"

#### 6.2 Monitor 페이지
1. **메트릭 대시보드**
   - 총 질문 수
   - 평균 응답 시간
   - 신뢰도 분포 (높음/중간/낮음)
   - 일일/주간/월간 추이 그래프

2. **피드백 통계**
   - 긍정 피드백 비율
   - 부정 피드백이 많은 질문 유형
   - 개선 필요 영역 식별

**강조 포인트**:
- "실시간 모니터링으로 시스템 성능 추적"
- "피드백 데이터로 지속적 품질 개선"
- "병목 구간 식별 및 최적화"

---

### 시나리오 7: 피드백 루프 (3분)

#### 데모 순서
1. **질문 입력 및 답변 확인**
   - 질문: "가스 배관 용접 기준은?"
   - 답변 받기

2. **긍정 피드백 (Thumbs Up)**
   - 👍 버튼 클릭
   - "피드백 감사합니다" 메시지 표시

3. **부정 피드백 (Thumbs Down)**
   - 다른 질문 시도: "비상 연락망은?"
   - 👎 버튼 클릭
   - 피드백 코멘트 입력: "출처가 명확하지 않음"

4. **피드백 데이터 확인**
   - Monitor 페이지로 이동
   - 최근 피드백 목록 확인
   - 부정 피드백 분석

**강조 포인트**:
- "사용자 피드백 수집으로 모델 개선"
- "문제 있는 답변 패턴 자동 식별"
- "향후 파인튜닝 데이터로 활용 가능"

---

## 3. 주요 강조 포인트

### 3.1 실시간 스트리밍 응답
- **SSE 기반 스트리밍**: 토큰이 생성되는 즉시 전달
- **사용자 경험 향상**: 응답 대기 시간 체감 감소
- **중단 기능**: 언제든 응답 생성 중단 가능

### 3.2 출처 근거 표시 (투명성)
- **문서명 + 페이지/섹션**: 정확한 출처 위치 표시
- **청크 하이라이트**: 해당 텍스트 구간 강조
- **다중 출처 지원**: 여러 문서에서 정보 통합 시 모두 표시

### 3.3 신뢰도 점수 + 안전장치
- **3단계 신뢰도**: 높음(녹색), 중간(노란색), 낮음(빨간색)
- **계산 방식**: Top-3 청크 유사도 평균 + 일관성 패널티
- **안전장치**: 낮은 신뢰도 시 경고 메시지 + 답변 가능 범위 안내

### 3.4 모델 교체 용이성
- **3개 Provider 지원**: Ollama, OpenAI, Anthropic
- **즉시 전환**: 설정 변경만으로 코드 수정 없이 교체
- **성능 비교**: 동일 질문으로 모델 간 벤치마킹 가능

### 3.5 한국어 최적화
- **bge-m3 임베딩 모델**: 다국어 지원, 한국어 성능 우수
- **한국어 프롬프트**: System Prompt 한국어 최적화
- **자연스러운 답변**: 존댓말, 단계별 설명 등 한국어 문화 고려

### 3.6 망분리 환경 대응
- **Ollama 로컬 모델**: 인터넷 없이 on-premise 실행
- **vLLM 지원**: 대규모 모델 고속 추론
- **완전 자체 호스팅**: 외부 API 의존성 제거 가능

### 3.7 확장 가능한 아키텍처
- **MCP Tools**: 외부 도구 통합 (계산기, 날씨 등)
- **Agent Builder**: 복잡한 작업을 단계별 Agent로 분해
- **모듈형 설계**: RAG → Router → Agent 단계별 확장

---

## 4. FAQ / 예상 질문 대응

### Q1: 왜 여러 LLM 모델을 교체할 수 있어야 하나요?
**A**:
- **망분리 환경 대응**: 인터넷 차단 환경에서는 Ollama/vLLM 로컬 모델 필수
- **비용 절감**: OpenAI/Claude는 강력하지만 고비용 → 단순 질문은 로컬 모델로 처리
- **성능 비교**: 동일 질문으로 모델 간 벤치마킹하여 최적 모델 선택
- **종속성 회피**: 특정 벤더에 종속되지 않고 언제든 전환 가능

### Q2: 신뢰도 점수는 어떻게 계산되나요?
**A**:
```
신뢰도 = (Top-3 청크 유사도 평균) - (일관성 패널티)
```
- **유사도 계산**: 질문 임베딩 vs 문서 청크 임베딩의 코사인 유사도
- **일관성 패널티**: Top-3 청크의 유사도 편차가 크면 감점
- **임계값**:
  - 높음: 0.7 이상 (녹색)
  - 중간: 0.4 ~ 0.7 (노란색)
  - 낮음: 0.4 미만 (빨간색, 경고)

### Q3: 보안은 어떻게 관리하나요?
**A**:
- **망분리 지원**: Ollama/vLLM 로컬 모델로 완전 폐쇄망 운영 가능
- **데이터 암호화**: ChromaDB 데이터 암호화 지원 (설정 시)
- **JWT 인증** (향후): 사용자 인증 및 권한 관리
- **API Key 관리**: 환경 변수로 민감 정보 분리 (`.env` 파일)
- **감사 로그**: 모든 질문과 답변 기록 (Monitor 페이지)

### Q4: 실제 데이터로 전환할 때 추가 작업이 필요한가요?
**A**:
- **거의 없음**: 샘플 문서 대신 실제 문서만 인제스트하면 즉시 사용 가능
- **필요한 작업**:
  1. 실제 문서를 `/backend/data/documents/` 폴더에 복사
  2. Documents 페이지에서 업로드 또는 API 호출로 인제스트
  3. (선택) System Prompt를 실제 업무에 맞게 수정
- **자동 처리**: 청킹, 임베딩, 벡터 DB 저장 모두 자동

### Q5: 동시 사용자를 얼마나 처리할 수 있나요?
**A**:
- **FastAPI Async**: 비동기 처리로 수천 개 동시 요청 처리 가능
- **수평 확장**: 여러 Backend 인스턴스를 Load Balancer 뒤에 배치
- **벡터 DB 확장**: ChromaDB → Qdrant/Weaviate 등 분산 DB로 전환 가능
- **캐싱**: 자주 묻는 질문은 Redis 캐싱으로 응답 속도 10배 향상
- **예상 처리량**: 단일 서버에서 100~200 QPS (초당 질문 수)

### Q6: 기존 사내 시스템과 통합할 수 있나요?
**A**:
- **REST API**: 모든 기능이 REST API로 제공 → 어떤 시스템과도 통합 가능
- **SSO 연동** (향후): LDAP, OAuth2, SAML 등 기업 인증 시스템 연동
- **Webhook**: 특정 이벤트 발생 시 외부 시스템에 알림 전송
- **MCP Tools**: 사내 ERP, 설비 관리 시스템 등을 Tool로 등록하여 Agent가 직접 호출

### Q7: 답변이 틀렸을 때 어떻게 개선하나요?
**A**:
- **피드백 수집**: 사용자가 👎 버튼으로 부정 피드백 + 코멘트 남김
- **데이터 분석**: Monitor 페이지에서 부정 피드백 패턴 분석
- **개선 방법**:
  1. **문서 보완**: 해당 주제의 문서가 누락되었다면 추가 인제스트
  2. **프롬프트 조정**: System Prompt를 수정하여 답변 스타일 개선
  3. **모델 교체**: 더 강력한 LLM으로 전환 (Ollama → GPT-4)
  4. **파인튜닝** (향후): 피드백 데이터로 모델 fine-tuning

---

## 5. 트러블슈팅

### 이슈 1: Backend 연결 안 됨
**증상**: Frontend에서 "서버 연결 실패" 에러

**해결 방법**:
1. Backend 서버가 실행 중인지 확인:
   ```bash
   curl http://localhost:8000/health
   ```
2. `.env` 파일에서 포트 확인:
   ```
   PORT=8000
   ```
3. 방화벽에서 8000 포트 허용 확인

### 이슈 2: 임베딩 모델 로드가 느림
**증상**: 첫 질문 시 10~20초 대기

**원인**: `BAAI/bge-m3` 모델 첫 다운로드 (약 1.5GB)

**해결 방법**:
- **정상 동작**: 첫 요청 시에만 느리고, 이후 캐시됨
- **사전 다운로드**: Backend 시작 시 자동 다운로드 (warmup)
- **로컬 경로 지정**: 이미 다운로드된 모델 경로 사용
  ```
  EMBEDDING_MODEL=/path/to/bge-m3
  ```

### 이슈 3: ChromaDB 오류
**증상**: `sqlite3.OperationalError: database is locked`

**해결 방법**:
1. Backend 서버 중지
2. ChromaDB 폴더 삭제:
   ```bash
   rm -rf /Users/blockmeta/Desktop/workspace/flux-rag/backend/data/chroma_db
   ```
3. Backend 재시작 후 문서 재인제스트

### 이슈 4: LLM 응답이 없음
**증상**: 질문 후 무한 로딩

**해결 방법**:
1. `.env` 파일에서 Provider 설정 확인:
   ```
   LLM_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   ```
2. Ollama 서버 실행 확인:
   ```bash
   curl http://localhost:11434/api/tags
   ```
3. OpenAI/Anthropic 사용 시 API Key 확인:
   ```
   OPENAI_API_KEY=sk-...
   ANTHROPIC_API_KEY=sk-ant-...
   ```

### 이슈 5: 한국어 답변이 부자연스러움
**증상**: 문법 오류, 어색한 표현

**해결 방법**:
1. System Prompt 수정 (Admin 페이지):
   ```
   당신은 한국가스기술공사의 AI 어시스턴트입니다.
   공손하고 전문적인 어투로 답변해 주세요.
   모든 답변은 존댓말을 사용하세요.
   ```
2. 더 강력한 한국어 모델로 교체:
   - Ollama: `llama3-korean`, `solar-10.7b`
   - OpenAI: `gpt-4`
   - Anthropic: `claude-3-opus`

---

## 6. 시연 후 다음 단계

### 6.1 즉시 가능한 작업
- [ ] 실제 문서 5~10개 인제스트하여 pilot 테스트
- [ ] 주요 사용자 10명과 2주간 베타 테스트
- [ ] 피드백 수집 및 프롬프트/모델 최적화

### 6.2 단기 확장 (1~2개월)
- [ ] SSO 연동 (LDAP/OAuth2)
- [ ] 사내 ERP 시스템 MCP Tool 연동
- [ ] 멀티모달 지원 (이미지, 도면 분석)
- [ ] 음성 질의응답 (STT/TTS)

### 6.3 중장기 확장 (3~6개월)
- [ ] Agent Builder로 복잡한 업무 자동화
  - 예: "지난달 이상사항을 분석하고 개선 보고서를 작성해줘"
- [ ] 파인튜닝: 한국가스기술공사 특화 모델 구축
- [ ] 멀티테넌트: 여러 부서/지사별 독립 문서 관리
- [ ] 모바일 앱 개발 (React Native)

---

## 7. 연락처 및 지원

**기술 문의**: flux-rag-support@example.com
**문서**: https://github.com/your-org/flux-rag/docs
**데모 영상**: https://youtu.be/demo-video-link

---

**시연을 마치며**:
"flux-rag는 단순한 질의응답 시스템이 아닙니다.
한국가스기술공사의 안전 문서와 지식을 지능적으로 연결하여,
직원 여러분의 업무 효율을 획기적으로 높이고,
안전 관리의 정확성을 강화하는 플랫폼입니다.

오늘 시연한 기능은 시작에 불과하며,
MCP Tools와 Agent Builder를 통해
무한한 확장 가능성을 가지고 있습니다.

함께 만들어가는 지능형 안전 관리 플랫폼, flux-rag입니다."
